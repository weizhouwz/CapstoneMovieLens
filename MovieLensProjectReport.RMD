---
title: "A Movie Recommendation System Using MovieLens Data"
output: pdf_document
---

## I. Introduction

Today machine learning is widely used in many industries. For instance, companies like Amazon use it to explore consumer behavior and target their products to a specific audience. A movie recommendation system is one of the applications built on top of machine learning algorithms. It is used to predict what rating a given user will assign to a movie, and ultimately it aims to provide users with movie recommendations based on their preferences and viewing history.

This report describes how I built a revised movie recommendation system based on the initial model discussed in the Edx Machine Learning course. The data used to train this system comes from the MovieLens 10M dataset, which was collected by GroupLens Research. It includes 10 million movie ratings and is publicly available for free download. The revised movie recommendation system introduces two new factors, the genres effect and the time effect, in addition to the movie and user effects of the original model. The performance of the new model is then evaluated using the RMSE score. It is compared against the RMSE of the original model with the goal to reduce it to be less than 0.8649.

## II. Methods and Analysis

#### 1. Data Processing and Exploration


GroupLens Research has collected various movie rating datasets and make them available on the MovieLens web site. The 10M version of the MovieLens dataset was selected for this project instead of other versions for computation purposes. The MovieLens 10M dataset can be found [here](https://grouplens.org/datasets/movielens/10m/).

The 10M data was downloaded from the MovieLens web site and then partitioned into two datasets, the edx dataset that was used to create the new model and the validation dataset that was used as the final hold-out test set to evaluate different algorithms. The validation dataset was supposed to be 10% of the entire 10M dataset. However, in order to make sure users and movies in the validation set were also in the edx set, an additional step was performed. As a result, the final count was 999,999, not 1,000,005.
```{r include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

# explore the dimension of the validation set
dim(validation)
```

A preliminary review of the edx dataset indicates it contains roughly 9 million records (N=9,000,055) with almost 70,000 unique users (N=69,878) and over 10,000 movies (N=10,677). It also includes movie ratings ranging from 0.5 to 5.
```{r include=FALSE}
# explore the dimension of the edx set, report unique counts of users and movies as well as the range of ratings
dim(edx)
n_distinct(edx$userId)
n_distinct(edx$movieId)
min(edx$rating)
max(edx$rating)
```

Here is the distribution of movie ratings in the edx dataset.

```{r echo=FALSE}
# explore the distribution of ratings
hist(edx$rating, main="Distrubution of Ratings", xlab="Rating", col="skyblue")
```

And here is a sample of the data itself. We can see it has altogether 6 variables including the 3 variables mentioned above.
```{r echo=FALSE}
# see the first 6 rows of the data
head(edx)
```


The timestamp variable in the dataset reports the date and time when a user rated a movie. However, it was not presented in a format easy to interpret, so it was converted using the lubridate package. In addition, a new variable was created based on this variable to reflect the year a movie was rated. With this new variable, I was able to calculate the difference in years between the movie debut year and when it was rated. This value was stored in the yrdiff variable and was used later in the model.
Similar data processing steps were executed on the validation set, too.
```{r include=FALSE}
# convert timestamp and create the difference in years variable in edx and validation sets
library(lubridate)
edx <- edx %>% mutate(timestamp=as_datetime(timestamp), yr=year(timestamp)) %>% extract(title, c("movietitle", "movieyear"), regex="^(.*) \\(([0-9]*)\\)$", remove=F) %>% mutate(yrdiff=ifelse(yr-as.numeric(movieyear)<0,0,yr-as.numeric(movieyear)))
validation <- validation %>% mutate(timestamp=as_datetime(timestamp), yr=year(timestamp)) %>% extract(title, c("movietitle", "movieyear"), regex="^(.*) \\(([0-9]*)\\)$", remove=F) %>% mutate(yrdiff=ifelse(yr-as.numeric(movieyear)<0,0,yr-as.numeric(movieyear)))
```
 
#### 2. Model Development


To build the new model, the edx dataset was further partitioned into two more datasets, the training dataset and the test dataset, with the test data to evaluate the model fit on the training data while tuning model parameters. It's common practice that the test dataset and the validation dataset have approximately the same number of records, therefore the test dataset was created to be 11% of the edx dataset. Again an extra step was performed to make sure users and movies in the test dataset were also in the training dataset. The final count of the test set was 989,991, which was very close to the number in the validation set.
```{r include=FALSE}
##########################################################
# Create training and test datasets
##########################################################

# set seed to 2
set.seed(2,sample.kind="Rounding")

# create training and test sets with test set being 11% of the edx set
ind <- createDataPartition(y=edx$rating, times=1, p=0.11, list=FALSE)
traindata <- edx[-ind,]
tmp <- edx[ind,]

# make sure userId and movieId in test set are also in training set
testdata <- tmp %>% semi_join(traindata, by="movieId") %>% semi_join(traindata, by="userId")

# check the number of records in the test set
nrow(testdata)

# add rows removed back into the training set
extra <- anti_join(tmp, testdata)
traindata <- rbind(traindata, extra)
```

Since the goal of this project was to improve the performance of the original model, it was natural for me to build the original model first and calculate its RMSE as the benchmark. The test dataset was used to find the best value for the regularization parameter given the original model, and then the model was applied to the validation dataset to report the final RMSE from that data.
```{r include=FALSE}
#############################
##### 1. Original Model #####

# create the original model and tune it to find the best value for lambda
lambdas <- seq(0, 10, 0.25)
rmses_orig <- sapply(lambdas, function(l){
	mu <- mean(traindata$rating)
	b_i <- traindata %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+l))
	b_u <- traindata %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+l))
	predicted_ratings <- testdata %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% mutate(pred = mu + b_i + b_u) %>% pull(pred)
	sqrt(mean((testdata$rating - predicted_ratings)^2))
})
min(rmses_orig)
lambda_orig <- lambdas[which.min(rmses_orig)]
lambda_orig

# create the model with the best lambda on the edx set and calculate the RMSE using the validation set
lambda <- lambda_orig
mu_edx <- mean(edx$rating)
b_i <- edx %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu_edx)/(n()+lambda))
b_u <- edx %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu_edx)/(n()+lambda))
r_hat <- validation %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% mutate(pred = mu_edx + b_i + b_u) %>% pull(pred)
rmse_orig <- sqrt(mean((validation$rating - r_hat)^2))
```

The next step was to identify additional variables that could be used to enhance the model. When reviewing the training dataset, I noticed some variables other than user and movie that potentially could be candidates for the new model. The first one was genres. As we know, differe movie genres appeal to different audiences. The person who like certain types of movie, for example, comedy, may rank comedy movies higher than other genres. Therefore the genres variable was added to my first model, New Model I. From the Edx course, I learned it was common to use 5 or 10 as the value for the regularization parameter, lamda, so I decided to test this particular parameter ranging from 0 to 10. It turned out that the value of 4.75 gave the minimum RMSE for the model using the movie, user and genres effects. Consequently, this value was used in New Model I, which was then applied to the validation set to calculate the RMSE and compare that to the benchmark RMSE.
```{r include=FALSE}
#############################
##### 2. New Model I #####

# check the genres effect using lambda from the original model
lambda <- lambda_orig
mu <- mean(traindata$rating)
b_i <- traindata %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+lambda))
b_u <- traindata %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
b_g <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+lambda))

# explore the distribution of the genres effect
hist(b_g$b_g, main="Distribution of Genres Effect", xlab="Genres Effect", col="skyblue")

# create New Model I and tune it to find the best value for lambda
lambdas <- seq(0, 10, 0.25)
rmses_m1 <- sapply(lambdas, function(l){
	mu <- mean(traindata$rating)
	b_i <- traindata %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+l))
	b_u <- traindata %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+l))
	b_g <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
	predicted_ratings <- testdata %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% left_join(b_g, by="genres") %>% mutate(pred = mu + b_i + b_u + b_g) %>% pull(pred)
	sqrt(mean((testdata$rating - predicted_ratings)^2))
})
min(rmses_m1)
lambda_m1 <- lambdas[which.min(rmses_m1)]
lambda_m1

# create New Model I with the best lambda on the edx set and calculate the RMSE using the validation set
lambda <- lambda_m1
mu_edx <- mean(edx$rating)
b_i <- edx %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu_edx)/(n()+lambda))
b_u <- edx %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu_edx)/(n()+lambda))
b_g <- edx %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu_edx)/(n()+lambda))
r_hat <- validation %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% left_join(b_g, by="genres") %>% mutate(pred = mu_edx + b_i + b_u + b_g) %>% pull(pred)
rmse_m1 <- sqrt(mean((validation$rating - r_hat)^2))
```

The second variable that was added to my model was the difference in years. Movie ratings can change over time. People seem to be critical or harsh on recent movies while they tend to be more lenient with older movies. This pattern can be observed in the following chart.

```{r echo=FALSE}
# explore relationship between difference in years and average rating
edx %>% group_by(yrdiff) %>% summarize(avg=mean(rating)) %>% ggplot(aes(yrdiff, avg)) + geom_point(color="blue") + ggtitle("Plot Average Rating by Difference in Years") + xlab("Difference in Years") + ylab("Average Rating")
```


Given this trend, the time effect, which was measured as the difference in years (i.e. the difference between the movie debut year and the year when it was rated), was introduced to the model, New Model II. Again, a similiar approach was adopted as New Model I, with the test set to identify the best value for the regularization parameter and then the final model was applied to the validation set to compute the RMSE of the model.
```{r include=FALSE}
#############################
##### 3. New Model II #####

# check the time effect using lambda from the original model
lambda <- lambda_orig
mu <- mean(traindata$rating)
b_i <- traindata %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+lambda))
b_u <- traindata %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
b_g <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+lambda))
b_y <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% left_join(b_g, by="genres") %>% group_by(yrdiff) %>% summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/(n()+lambda))

# explore the distribution of the time effect
hist(b_y$b_y, main="Distribution of Time Effect", xlab="Time Effect", col="skyblue")

# create New Model II and tune it to find the best value for lambda
lambdas <- seq(0, 10, 0.25)
rmses_m2 <- sapply(lambdas, function(l){
	mu <- mean(traindata$rating)
	b_i <- traindata %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+l))
	b_u <- traindata %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+l))
	b_g <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
	b_y <- traindata %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% left_join(b_g, by="genres") %>% group_by(yrdiff) %>% summarize(b_y = sum(rating - b_i - b_u - b_g - mu)/(n()+l))
	predicted_ratings <- testdata %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% left_join(b_g, by="genres") %>% left_join(b_y, by="yrdiff") %>% mutate(pred = mu + b_i + b_u + b_g + b_y) %>% pull(pred)
	sqrt(mean((testdata$rating - predicted_ratings)^2))
})
min(rmses_m2)
lambda_m2 <- lambdas[which.min(rmses_m2)]
lambda_m2

# create New Model II with the best lambda on the edx set and calculate the RMSE using the validation set
lambda <- lambda_m2
mu_edx <- mean(edx$rating)
b_i <- edx %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu_edx)/(n()+lambda))
b_u <- edx %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu_edx)/(n()+lambda))
b_g <- edx %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% group_by(genres) %>% summarize(b_g = sum(rating - b_i - b_u - mu_edx)/(n()+lambda))
b_y <- edx %>% left_join(b_i, by="movieId") %>% left_join(b_u, by="userId") %>% left_join(b_g, by="genres") %>% group_by(yrdiff) %>% summarize(b_y = sum(rating - b_i - b_u - b_g - mu_edx)/(n()+lambda))
r_hat <- validation %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% left_join(b_g, by="genres") %>% left_join(b_y, by="yrdiff") %>% mutate(pred = mu_edx + b_i + b_u + b_g + b_y) %>% pull(pred)
rmse_m2 <- sqrt(mean((validation$rating - r_hat)^2))
```

## III. Results

The original model has a RMSE of **0.8648201** (see below), which serves as the baseline to compare against new models.
```{r echo=FALSE}
# print the RMSE of the original model
cat("RMSE of the original model: ", rmse_orig, "\n")
```

The first model (New Model I) introduces a new variable, the genres effect, which is different from the original model that uses the movie and user effects only. This model has a RMSE of **0.8644514** (see below) when run on the validation set. This is lower than the RMSE of the original model and proves that the genres effect does improve the model performance and is in fact a useful predictor for movie ratings.
```{r echo=FALSE}
# print the RMSE of New Model I
cat("RMSE of New Model I: ", rmse_m1, "\n")
```

The second model (New Model II) adds one more variable to measure the time effect on movie ratings. The final RMSE on the validation set is **0.8640185** (see below). Again this model further reduces the RMSE and improves the model performance quite a bit. Given it has the lowest RMSE among the three, this is the final model that I select to predict movie ratings.
```{r echo=FALSE}
# print the RMSE of New Model II
cat("RMSE of New Model II: ", rmse_m2, "\n")
```

## IV. Conclusion

Movie recommendation systems are quite common in today's world. They are often used to create personalized content for each user and are of particular importance to companies like Netflix. Each system may use different variables and implement different algorithms to achieve the best results. The model discussed in this report is developed based on the original model from the Edx Machine Learning course. It introduces two additional factors, the genres effect and the time effect, in an attempt to improve the model performance. The results indicate both of those two factors are useful predictors and contribute to reducing the RMSE of the final model. Of course these are not the sole factors that may affect movie ratings. User demographics and Movie characteristics are likely to play an important role, too. Even with the existing factors in my model, there are also possibilities for potential improvement. For example, the genres effect can be further examined by splitting the individual genre into separate categories. Future research in those areas can further enhance the movie recommendation system and provide more accurate predictions.
